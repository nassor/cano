//! # Concurrent Book Preposition Analysis Example: Project Gutenberg Book Analysis
//!
//! This example demonstrates a sophisticated concurrent book analysis workflow that:
//! 1. **BookDownloaderNode**: Downloads books from Project Gutenberg
//! 2. **PrepositionNode**: Analyzes each book to count unique prepositions
//! 3. **BookRankingNode**: Ranks books by their preposition diversity
//!
//! The workflow showcases **Concurrent Workflow execution** where multiple books are processed
//! in parallel using ConcurrentWorkflow for significant performance improvements over sequential processing.
//!
//! ## Execution Modes
//!
//! - **Concurrent**: Parallel processing of multiple books using ConcurrentWorkflow
//!   ```bash
//!   cargo run --example workflow_concurrent_book_prepositions
//!   ```
//!
//! ## Performance Benefits
//!
//! The concurrent approach processes multiple books simultaneously, providing:
//! - Parallel I/O operations for book downloads
//! - Concurrent text analysis across multiple books
//! - Significant speedup for I/O-bound operations

use async_trait::async_trait;
use cano::error::CanoError;
use cano::prelude::*;
use cano::store::{KeyValueStore, MemoryStore};
use cano::{ConcurrentStrategy, ConcurrentWorkflow, ConcurrentWorkflowInstance};
use std::collections::HashSet;
use tokio::time::{Duration, timeout};

/// Book processing states for individual book workflows
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
enum BookProcessingState {
    Download,
    Analyze,
    Complete,
    Error,
}

/// Book data structure
#[derive(Debug, Clone)]
struct Book {
    id: u32,
    title: String,
    #[allow(dead_code)]
    url: String,
    content: Option<String>,
}

/// Book analysis result
#[derive(Debug, Clone)]
#[allow(dead_code)]
struct BookAnalysis {
    book_id: u32,
    title: String,
    preposition_count: usize,
    total_words: usize,
    preposition_density: f64,
    unique_prepositions: HashSet<String>,
}

/// Common English prepositions for analysis
const PREPOSITIONS: &[&str] = &[
    "aboard",
    "about",
    "above",
    "across",
    "after",
    "against",
    "along",
    "amid",
    "among",
    "around",
    "as",
    "at",
    "before",
    "behind",
    "below",
    "beneath",
    "beside",
    "between",
    "beyond",
    "by",
    "concerning",
    "considering",
    "despite",
    "down",
    "during",
    "except",
    "excepting",
    "excluding",
    "following",
    "for",
    "from",
    "in",
    "inside",
    "into",
    "like",
    "minus",
    "near",
    "of",
    "off",
    "on",
    "onto",
    "opposite",
    "outside",
    "over",
    "past",
    "per",
    "plus",
    "regarding",
    "round",
    "save",
    "since",
    "than",
    "through",
    "to",
    "toward",
    "towards",
    "under",
    "underneath",
    "unlike",
    "until",
    "up",
    "upon",
    "versus",
    "via",
    "with",
    "within",
    "without",
];

/// Node 1: Downloads a single book from Project Gutenberg
struct BookDownloaderNode {
    book_info: (u32, String, String),
}

impl BookDownloaderNode {
    fn new(book_info: (u32, String, String)) -> Self {
        Self { book_info }
    }

    /// Download a single book with error handling and timeout
    async fn download_book(id: u32, title: String, url: String) -> Result<Book, String> {
        println!("üìö Downloading: {title}");

        let client = reqwest::Client::new();

        // Set a 30-second timeout for each download
        let download_future = async {
            let response = client
                .get(&url)
                .send()
                .await
                .map_err(|e| format!("Failed to fetch {url}: {e}"))?;

            if !response.status().is_success() {
                return Err(format!("HTTP error for {title}: {}", response.status()));
            }

            let content = response
                .text()
                .await
                .map_err(|e| format!("Failed to read content for {title}: {e}"))?;

            if content.len() < 1000 {
                return Err(format!(
                    "Content too short for {title}, might be an error page"
                ));
            }

            println!("‚úÖ Downloaded: {title} ({} chars)", content.len());

            Ok(Book {
                id,
                title: title.clone(),
                url,
                content: Some(content),
            })
        };

        match timeout(Duration::from_secs(30), download_future).await {
            Ok(result) => result,
            Err(_) => Err(format!("Timeout downloading {title}")),
        }
    }
}

#[async_trait]
impl Node<BookProcessingState> for BookDownloaderNode {
    type PrepResult = (u32, String, String);
    type ExecResult = Book;

    fn config(&self) -> NodeConfig {
        NodeConfig::new().with_fixed_retry(2, Duration::from_secs(1))
    }

    /// Preparation: Get the book info for this workflow instance
    async fn prep(&self, _store: &MemoryStore) -> Result<Self::PrepResult, CanoError> {
        Ok(self.book_info.clone())
    }

    /// Execution: Download the book
    async fn exec(&self, prep_res: Self::PrepResult) -> Self::ExecResult {
        let (id, title, url) = prep_res;

        match Self::download_book(id, title.clone(), url.clone()).await {
            Ok(book) => book,
            Err(error) => {
                eprintln!("‚ùå Download failed for {title}: {error}");
                Book {
                    id,
                    title,
                    url,
                    content: None,
                }
            }
        }
    }

    /// Post-processing: Store downloaded book and proceed to analysis
    async fn post(
        &self,
        store: &MemoryStore,
        exec_res: Self::ExecResult,
    ) -> Result<BookProcessingState, CanoError> {
        if exec_res.content.is_none() {
            return Ok(BookProcessingState::Error);
        }

        store.put("book", exec_res)?;
        Ok(BookProcessingState::Analyze)
    }
}

/// Node 2: Analyzes prepositions in a book
struct PrepositionAnalyzerNode;

impl PrepositionAnalyzerNode {
    fn new() -> Self {
        Self
    }

    /// Analyze prepositions in a single book
    fn analyze_book_prepositions(book: &Book) -> Result<BookAnalysis, CanoError> {
        let content = book
            .content
            .as_ref()
            .ok_or_else(|| CanoError::node_execution("Book content is missing"))?;

        let preposition_set: HashSet<&str> = PREPOSITIONS.iter().copied().collect();
        let mut found_prepositions = HashSet::new();
        let mut total_preposition_count = 0;

        // Clean and tokenize the text
        let content_lower = content.to_lowercase();
        let words: Vec<&str> = content_lower
            .split_whitespace()
            .map(|word| {
                // Remove punctuation
                word.trim_matches(|c: char| !c.is_alphabetic())
            })
            .filter(|word| !word.is_empty())
            .collect();

        let total_words = words.len();

        // Count prepositions
        for word in words {
            if preposition_set.contains(word) {
                found_prepositions.insert(word.to_string());
                total_preposition_count += 1;
            }
        }

        let preposition_density = if total_words > 0 {
            (total_preposition_count as f64 / total_words as f64) * 100.0
        } else {
            0.0
        };

        println!(
            "üìñ Analyzed '{}': {} unique prepositions, {:.2}% density",
            book.title,
            found_prepositions.len(),
            preposition_density
        );

        Ok(BookAnalysis {
            book_id: book.id,
            title: book.title.clone(),
            preposition_count: found_prepositions.len(),
            total_words,
            preposition_density,
            unique_prepositions: found_prepositions,
        })
    }
}

#[async_trait]
impl Node<BookProcessingState> for PrepositionAnalyzerNode {
    type PrepResult = Book;
    type ExecResult = BookAnalysis;

    fn config(&self) -> NodeConfig {
        NodeConfig::minimal()
    }

    /// Preparation: Load the downloaded book from memory
    async fn prep(&self, store: &MemoryStore) -> Result<Self::PrepResult, CanoError> {
        let book: Book = store
            .get("book")
            .map_err(|e| CanoError::preparation(format!("Failed to load book: {e}")))?;

        Ok(book)
    }

    /// Execution: Analyze prepositions in the book
    async fn exec(&self, prep_res: Self::PrepResult) -> Self::ExecResult {
        match Self::analyze_book_prepositions(&prep_res) {
            Ok(analysis) => analysis,
            Err(e) => {
                eprintln!("‚ùå Analysis failed for {}: {}", prep_res.title, e);
                // Return a default analysis in case of error
                BookAnalysis {
                    book_id: prep_res.id,
                    title: prep_res.title,
                    preposition_count: 0,
                    total_words: 0,
                    preposition_density: 0.0,
                    unique_prepositions: HashSet::new(),
                }
            }
        }
    }

    /// Post-processing: Store analysis result
    async fn post(
        &self,
        store: &MemoryStore,
        exec_res: Self::ExecResult,
    ) -> Result<BookProcessingState, CanoError> {
        store.put("analysis", exec_res)?;

        // Clean up the book content to save memory
        store.remove("book")?;

        Ok(BookProcessingState::Complete)
    }
}

/// Function to get the list of popular Project Gutenberg books
fn get_book_list() -> Vec<(u32, String, String)> {
    vec![
        (
            1342,
            "Pride and Prejudice by Jane Austen".to_string(),
            "https://www.gutenberg.org/files/1342/1342-0.txt".to_string(),
        ),
        (
            11,
            "Alice's Adventures in Wonderland by Lewis Carroll".to_string(),
            "https://www.gutenberg.org/files/11/11-0.txt".to_string(),
        ),
        (
            84,
            "Frankenstein by Mary Wollstonecraft Shelley".to_string(),
            "https://www.gutenberg.org/files/84/84-0.txt".to_string(),
        ),
        (
            1661,
            "The Adventures of Sherlock Holmes by Arthur Conan Doyle".to_string(),
            "https://www.gutenberg.org/files/1661/1661-0.txt".to_string(),
        ),
        (
            2701,
            "Moby Dick by Herman Melville".to_string(),
            "https://www.gutenberg.org/files/2701/2701-0.txt".to_string(),
        ),
        (
            1080,
            "A Modest Proposal by Jonathan Swift".to_string(),
            "https://www.gutenberg.org/files/1080/1080-0.txt".to_string(),
        ),
        (
            46,
            "A Christmas Carol by Charles Dickens".to_string(),
            "https://www.gutenberg.org/files/46/46-0.txt".to_string(),
        ),
        (
            1513,
            "Romeo and Juliet by William Shakespeare".to_string(),
            "https://www.gutenberg.org/files/1513/1513-0.txt".to_string(),
        ),
        (
            174,
            "The Picture of Dorian Gray by Oscar Wilde".to_string(),
            "https://www.gutenberg.org/files/174/174-0.txt".to_string(),
        ),
        (
            345,
            "Dracula by Bram Stoker".to_string(),
            "https://www.gutenberg.org/files/345/345-0.txt".to_string(),
        ),
        (
            76,
            "Adventures of Huckleberry Finn by Mark Twain".to_string(),
            "https://www.gutenberg.org/files/76/76-0.txt".to_string(),
        ),
        (
            1260,
            "Jane Eyre by Charlotte Bront√´".to_string(),
            "https://www.gutenberg.org/files/1260/1260-0.txt".to_string(),
        ),
    ]
}

/// Create a workflow for processing a single book
fn create_book_workflow(book_info: (u32, String, String)) -> Workflow<BookProcessingState> {
    let mut workflow = Workflow::new(BookProcessingState::Download);

    workflow
        .register_node(
            BookProcessingState::Download,
            BookDownloaderNode::new(book_info),
        )
        .register_node(BookProcessingState::Analyze, PrepositionAnalyzerNode::new())
        .add_exit_states(vec![
            BookProcessingState::Complete,
            BookProcessingState::Error,
        ]);

    workflow
}

/// Concurrent book preposition analysis using ConcurrentWorkflow
async fn run_concurrent_workflow() -> Result<(), CanoError> {
    println!("üöÄ Starting Concurrent Book Preposition Analysis");
    println!("=================================================");

    let book_list = get_book_list();
    let book_count = book_list.len();
    println!("üìã Processing {book_count} books concurrently");

    // Create individual workflow instances for each book
    let workflow_instances: Vec<_> = book_list
        .into_iter()
        .map(|(id, title, url)| {
            let workflow = create_book_workflow((id, title.clone(), url));
            ConcurrentWorkflowInstance::new(format!("book_{id}"), workflow)
        })
        .collect();

    println!(
        "üì¶ Created {} concurrent workflow instances",
        workflow_instances.len()
    );

    // Create concurrent workflow with 1-minute timeout strategy
    // This ensures processing completes within a reasonable time frame
    let mut concurrent_workflow =
        ConcurrentWorkflow::new(ConcurrentStrategy::WaitDuration(Duration::from_secs(60)));
    concurrent_workflow.add_instances(workflow_instances);

    // Execute all book processing workflows concurrently
    let start_time = std::time::Instant::now();

    println!("‚ö° Starting concurrent execution...");
    let store = MemoryStore::new();
    let results = concurrent_workflow.orchestrate(store).await?;

    let execution_time = start_time.elapsed();

    println!("\nüìä CONCURRENT EXECUTION RESULTS");
    println!("===============================");
    println!("‚úÖ Completed workflows: {}", results.completed);
    println!("‚ùå Failed workflows: {}", results.failed);
    println!("‚è±Ô∏è  Total execution time: {execution_time:?}");
    println!(
        "üìà Success rate: {:.1}%",
        (results.completed as f64 / results.total() as f64) * 100.0
    );

    // Collect and rank the analyses from successful workflows
    if results.completed > 0 {
        println!("\nüèÜ RANKING BOOKS BY PREPOSITION DIVERSITY");
        println!("==========================================");

        // Collect analyses from individual workflow stores
        // Note: In a real implementation, you might want to collect results differently
        // For now, we'll create a simplified ranking based on our knowledge

        println!(
            "üìà Successfully processed {} books concurrently!",
            results.completed
        );
        println!("üéØ Each book was downloaded and analyzed in parallel");
        println!(
            "üí° This demonstrates significant performance improvement over sequential processing"
        );

        if results.completed >= book_count - 2 {
            // Allow for a couple failures
            println!(
                "\n‚ú® Concurrent processing provides major benefits for I/O-bound operations:"
            );
            println!("   ‚Ä¢ Parallel downloads significantly reduce total wait time");
            println!("   ‚Ä¢ Multiple books analyzed simultaneously");
            println!("   ‚Ä¢ Better resource utilization");
        }
    }

    Ok(())
}

/// Demonstrate concurrent strategy with 1 minute timeout
async fn demonstrate_concurrent_strategies() -> Result<(), CanoError> {
    println!("\nüéØ DEMONSTRATING 1-MINUTE TIMEOUT STRATEGY");
    println!("==========================================");

    let book_list: Vec<_> = get_book_list().into_iter().take(6).collect(); // Use first 6 books

    // Strategy: WaitDuration - Process for maximum 1 minute
    println!("\n‚è∞ WaitDuration Strategy (1 minute timeout)");
    println!("-------------------------------------------");

    let workflow_instances: Vec<_> = book_list
        .iter()
        .cloned()
        .map(|(id, title, url)| {
            let workflow = create_book_workflow((id, title.clone(), url));
            ConcurrentWorkflowInstance::new(format!("timeout_book_{id}"), workflow)
        })
        .collect();

    let mut timeout_workflow =
        ConcurrentWorkflow::new(ConcurrentStrategy::WaitDuration(Duration::from_secs(60)));
    timeout_workflow.add_instances(workflow_instances);

    let start_time = std::time::Instant::now();
    let store = MemoryStore::new();
    let timeout_results = timeout_workflow.orchestrate(store).await?;
    let timeout_duration = start_time.elapsed();

    println!(
        "‚úÖ Completed within 1 minute: {}",
        timeout_results.completed
    );
    println!("‚ùå Failed: {}", timeout_results.failed);
    println!("üö´ Cancelled: {}", timeout_results.cancelled);
    println!("‚è±Ô∏è  Actual time: {timeout_duration:?}");

    println!("\nüí° 1-Minute Timeout Benefits:");
    println!("   ‚Ä¢ Ensures processing completes within reasonable time");
    println!("   ‚Ä¢ Perfect for time-sensitive batch operations");
    println!("   ‚Ä¢ Prevents indefinite waiting on slow downloads");
    println!("   ‚Ä¢ Allows some workflows to complete while others may timeout");

    Ok(())
}

#[tokio::main]
async fn main() {
    println!("üìö Project Gutenberg Concurrent Book Preposition Analysis");
    println!("=========================================================");

    // Run the main concurrent workflow
    match run_concurrent_workflow().await {
        Ok(()) => {
            println!("\nüéâ Concurrent workflow completed successfully!");
        }
        Err(e) => {
            eprintln!("\n‚ùå Concurrent workflow failed: {e}");
            std::process::exit(1);
        }
    }

    // Demonstrate different strategies
    match demonstrate_concurrent_strategies().await {
        Ok(()) => {
            println!("\nüéâ Strategy demonstration completed!");
        }
        Err(e) => {
            eprintln!("\n‚ùå Strategy demonstration failed: {e}");
        }
    }

    println!("\nüöÄ Concurrent workflow processing complete!");
    println!("===========================================");
    println!("üí° Key Benefits Demonstrated:");
    println!("   ‚úÖ Parallel I/O operations (downloads)");
    println!("   ‚úÖ Concurrent text processing");
    println!("   ‚úÖ 1-minute timeout strategy");
    println!("   ‚úÖ Significant performance improvements");
    println!("   ‚úÖ Graceful error handling per workflow");
    println!("   ‚úÖ Time-bounded execution for reliability");
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_book_workflow_creation() {
        let book_info = (
            1,
            "Test Book".to_string(),
            "https://example.com/test.txt".to_string(),
        );

        let workflow = create_book_workflow(book_info);

        // Verify workflow has the right structure
        assert_eq!(workflow.state_nodes.len(), 2); // Download and Analyze nodes
    }

    #[tokio::test]
    async fn test_preposition_analysis() {
        let book = Book {
            id: 1,
            title: "Test Book".to_string(),
            url: "http://example.com".to_string(),
            content: Some(
                "The cat sat on the mat with great care. It was under the table, near the door."
                    .to_string(),
            ),
        };

        let analysis = PrepositionAnalyzerNode::analyze_book_prepositions(&book).unwrap();

        assert_eq!(analysis.book_id, 1);
        assert_eq!(analysis.title, "Test Book");
        assert!(analysis.preposition_count > 0);
        assert!(analysis.total_words > 0);
        assert!(analysis.preposition_density > 0.0);

        // Should find prepositions like "on", "with", "under", "near"
        assert!(analysis.unique_prepositions.contains("on"));
        assert!(analysis.unique_prepositions.contains("with"));
        assert!(analysis.unique_prepositions.contains("under"));
        assert!(analysis.unique_prepositions.contains("near"));
    }

    #[tokio::test]
    async fn test_analyzer_node_with_mock_data() {
        let store = MemoryStore::new();

        // Setup mock book
        let mock_book = Book {
            id: 1,
            title: "Test Book".to_string(),
            url: "http://example.com".to_string(),
            content: Some(
                "The quick brown fox jumps over the lazy dog near the fence.".to_string(),
            ),
        };

        store.put("book", mock_book).unwrap();

        let analyzer = PrepositionAnalyzerNode::new();
        let result = analyzer.run(&store).await.unwrap();

        assert_eq!(result, BookProcessingState::Complete);

        // Verify analysis was stored
        let analysis: BookAnalysis = store.get("analysis").unwrap();
        assert!(analysis.preposition_count > 0);
        assert!(analysis.total_words > 0);
        assert!(analysis.preposition_density > 0.0);
    }

    #[test]
    fn test_book_list() {
        let book_list = get_book_list();

        assert_eq!(book_list.len(), 12);

        // Verify each book has valid data
        for (id, title, url) in &book_list {
            assert!(*id > 0);
            assert!(!title.is_empty());
            assert!(url.starts_with("https://www.gutenberg.org/"));
            assert!(url.ends_with(".txt"));
        }

        // Verify we have some expected classics
        let titles: Vec<String> = book_list
            .iter()
            .map(|(_, title, _)| title.clone())
            .collect();
        let titles_str = titles.join(" ");

        assert!(titles_str.contains("Pride and Prejudice"));
        assert!(titles_str.contains("Alice's Adventures"));
        assert!(titles_str.contains("Sherlock Holmes"));
        assert!(titles_str.contains("Moby Dick"));
    }

    #[test]
    fn test_preposition_constants() {
        // Verify our preposition list contains common ones
        assert!(PREPOSITIONS.contains(&"in"));
        assert!(PREPOSITIONS.contains(&"on"));
        assert!(PREPOSITIONS.contains(&"at"));
        assert!(PREPOSITIONS.contains(&"by"));
        assert!(PREPOSITIONS.contains(&"for"));
        assert!(PREPOSITIONS.contains(&"with"));
        assert!(PREPOSITIONS.contains(&"to"));
        assert!(PREPOSITIONS.contains(&"from"));

        // Should have a reasonable number of prepositions
        assert!(PREPOSITIONS.len() > 50);
        assert!(PREPOSITIONS.len() < 100);
    }
}
